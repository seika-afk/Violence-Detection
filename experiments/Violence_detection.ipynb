{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Ingestion\n"
      ],
      "metadata": {
        "id": "hJNL10ks5YvU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nToaReAKsc4X"
      },
      "outputs": [],
      "source": [
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "# formality 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d mohamedmustafa/real-life-violence-situations-dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRnbc5eA0IGN",
        "outputId": "5fb2c0a3-61a0-487b-d094-8cd34fa44337"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/mohamedmustafa/real-life-violence-situations-dataset\n",
            "License(s): copyright-authors\n",
            "Downloading real-life-violence-situations-dataset.zip to /content\n",
            "100% 3.58G/3.58G [00:50<00:00, 76.9MB/s]\n",
            "100% 3.58G/3.58G [00:50<00:00, 75.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import collections\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers"
      ],
      "metadata": {
        "id": "ZWTU5xw_0sbT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n"
      ],
      "metadata": {
        "id": "DSqDyo292Ek6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('/content/dataset.zip', 'r')\n",
        "zip_ref.extractall('/content/data')\n",
        "zip_ref.close()\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "afRZMZvu2NrK"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class(url):\n",
        "  classes=os.listdir(url)\n",
        "  return classes"
      ],
      "metadata": {
        "id": "p3OzF-vN2lG7"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "B-02Exto4ut8"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_files_with_class(url):\n",
        "  classes=get_class(url)\n",
        "  files=collections.defaultdict(list)\n",
        "  for class_ in classes:\n",
        "    paths=os.path.join(url,class_)\n",
        "    files[class_].extend(os.listdir(paths))\n",
        "  return files\n",
        "\n"
      ],
      "metadata": {
        "id": "5f8XInc82qJu"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url=\"/content/data/Real Life Violence Dataset\""
      ],
      "metadata": {
        "id": "PE2Sq6sf3Ruu"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files=get_files_with_class(dataset_url)"
      ],
      "metadata": {
        "id": "PgDCYS453jCh"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(files) # 2 CLasses : 1 violence and 1 non violence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft9kdXM944HQ",
        "outputId": "edf6becc-5cb1-4dc1-9bbd-32b850945f3a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(files['NonViolence']) # 1000 videos each"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAG7NuIZ5H4R",
        "outputId": "7494074e-8781-4375-83bd-2e6af908b4d7"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "6_KxZdQW5lkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import shutil  # to move files"
      ],
      "metadata": {
        "id": "grUIH2TV7ZRn"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDKiqznV7kxU",
        "outputId": "811fe705-a842-431e-b5d6-9fede3f5338a"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['Violence', 'NonViolence'])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_data(files,source,split_ratio):\n",
        "  classes=os.listdir(source)\n",
        "  print(classes)\n",
        "  for class_name in classes:\n",
        "    print(class_name)\n",
        "    #getting videos from path\n",
        "    class_path = os.path.join(source,str(class_name))\n",
        "    videos=[f for f in os.listdir(class_path) if f.endswith('.mp4')]\n",
        "    random.shuffle(videos)\n",
        "    # setting the split\n",
        "    split_point=int(len(videos)*split_ratio)\n",
        "    train_videos=videos[:split_point]\n",
        "    test_videos=videos[split_point:]\n",
        "\n",
        "    # passing from each file\n",
        "    for phase,file_list in [(\"train\",train_videos),(\"test\",test_videos)]:\n",
        "      destination=os.path.join(source,phase,class_name)\n",
        "      os.makedirs(destination,exist_ok=True)\n",
        "\n",
        "      for vid in file_list:\n",
        "        src=os.path.join(class_path,vid)\n",
        "        dest=os.path.join(destination,vid)\n",
        "        shutil.move(src,dest)\n",
        "\n"
      ],
      "metadata": {
        "id": "BXwoeujn5R9d"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "divide_data(files,'/content/data/Real Life Violence Dataset',0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm9OAgbl72U1",
        "outputId": "14598a3e-df89-47bb-9696-f477db548fa0"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Violence', 'NonViolence']\n",
            "Violence\n",
            "NonViolence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir(\"/content/data/Real Life Violence Dataset/test/Violence\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wX1GMgE920h",
        "outputId": "052d2139-ac01-432d-f0dc-ca4eea8082ae"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.rmtree(\"/content/data/real life violence situations\")"
      ],
      "metadata": {
        "id": "s1P3ggpE-mir"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  ---"
      ],
      "metadata": {
        "id": "QpAjHvvNAJ5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "metadata": {
        "id": "xVsFxoXyHOqV"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_frames(frame,output_size):\n",
        "  #setting frames\n",
        "  #padding and resizing\n",
        "  #output size\n",
        "  frame=tf.image.convert_image_dtype(frame,tf.float32)\n",
        "  frame=tf.image.resize_with_pad(frame,*output_size)\n",
        "  return frame"
      ],
      "metadata": {
        "id": "z8A6HvEI_WMy"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n",
        "  \"\"\"\n",
        "    Creates frames from each video file present for each category.\n",
        "\n",
        "    Args:\n",
        "      video_path: File path to the video.\n",
        "      n_frames: Number of frames to be created per video file.\n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
        "  \"\"\"\n",
        "  # Read each video frame by frame\n",
        "  result = []\n",
        "  src = cv2.VideoCapture(str(video_path))\n",
        "\n",
        "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "  need_length = 1 + (n_frames - 1) * frame_step\n",
        "\n",
        "  if need_length > video_length:\n",
        "    start = 0\n",
        "  else:\n",
        "    max_start = video_length - need_length\n",
        "    start = random.randint(0, max_start + 1)\n",
        "\n",
        "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
        "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
        "  ret, frame = src.read()\n",
        "  result.append(format_frames(frame, output_size))\n",
        "\n",
        "  for _ in range(n_frames - 1):\n",
        "    for _ in range(frame_step):\n",
        "      ret, frame = src.read()\n",
        "    if ret:\n",
        "      frame = format_frames(frame, output_size)\n",
        "      result.append(frame)\n",
        "    else:\n",
        "      result.append(np.zeros_like(result[0]))\n",
        "  src.release()\n",
        "  result = np.array(result)[..., [2, 1, 0]]\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "Eh-lkIOQAdoQ"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frame generator"
      ],
      "metadata": {
        "id": "F4p3w-n9AmIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FrameGenerator:\n",
        "  def __init__(self,path,n_frames,training=False):\n",
        "    self.path = path\n",
        "    self.n_frames = n_frames\n",
        "    self.training = training\n",
        "    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
        "    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
        "\n",
        "  def get_files_and_class_names(self):\n",
        "    video_paths=list(self.path.glob('*/*.mp4'))\n",
        "    classes=[p.parent.name for p in video_paths]\n",
        "    return video_paths,classes\n",
        "\n",
        "\n",
        "  def __call__(self):\n",
        "    video_paths, classes = self.get_files_and_class_names()\n",
        "\n",
        "    pairs = list(zip(video_paths, classes))\n",
        "\n",
        "    if self.training:\n",
        "      random.shuffle(pairs)\n",
        "\n",
        "    for path, name in pairs:\n",
        "      video_frames = frames_from_video_file(path, self.n_frames)\n",
        "      label = self.class_ids_for_name[name] # Encode labels\n",
        "      yield video_frames, label"
      ],
      "metadata": {
        "id": "49myynumAiyi"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "# Create the training set\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(10, 224, 224, 3), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(),                  dtype=tf.int32)\n",
        ")\n",
        "fg_train = FrameGenerator(Path('/content/data/Real Life Violence Dataset/train'), n_frames=10, training=True)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_generator(fg_train, output_signature=output_signature)\n"
      ],
      "metadata": {
        "id": "712PXM7IAoK8"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for frames,labels in train_ds.take(10):\n",
        "  print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6-tfTDsBDRD",
        "outputId": "5bb6af60-a424-4432-f288-7ec2b893f85b"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#validation dataset\n",
        "\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(10, 224, 224, 3), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(),                  dtype=tf.int32)\n",
        ")\n",
        "\n",
        "fg_val = FrameGenerator(Path('/content/data/Real Life Violence Dataset/test'), n_frames=10, training=False)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_generator(fg_val, output_signature=output_signature)\n"
      ],
      "metadata": {
        "id": "leYjL9RHBHnY"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_ds = train_ds.batch(2)\n",
        "val_ds = val_ds.batch(2)"
      ],
      "metadata": {
        "id": "nsIk03IkCEeP"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation"
      ],
      "metadata": {
        "id": "44R7N87WCxIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRansfer Learning:\n",
        "    #fine tuning"
      ],
      "metadata": {
        "id": "uCHd7zvtCIlD"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "phBgubDpEf2G"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models=[\n",
        "    tensorflow.keras.applications.EfficientNetB0,\n",
        "    tensorflow.keras.applications.MobileNetV2\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "r2O0ty3YCXlD"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shape=(224,224,3)"
      ],
      "metadata": {
        "id": "IXEzPGzeEy4-"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trying without fine tuning\n",
        "\n",
        "for model_fn in models:\n",
        "\n",
        "  base_model=model_fn(include_top=False,pooling='avg',input_shape=(224,224,3))\n",
        "  base_model.trainable=False\n",
        "  print(f\" Using base model: {model_fn.__name__}\")\n",
        "  model =tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(10, 224, 224, 3)),\n",
        "    tf.keras.layers.Rescaling(scale=1./255),\n",
        "    tf.keras.layers.TimeDistributed(base_model ),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(2)\n",
        "\n",
        "\n",
        "  ])\n",
        "  model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
        "  model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=3,\n",
        "    steps_per_epoch=10,\n",
        "    validation_steps=2,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K9k2NvGEbAg",
        "outputId": "6e391660-8da3-41cd-85fa-3811cf85cadb"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Using base model: EfficientNetB0\n",
            "Epoch 1/3\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 6s/step - accuracy: 0.5339 - loss: 0.7052 - val_accuracy: 0.0000e+00 - val_loss: 0.9165\n",
            "Epoch 2/3\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2s/step - accuracy: 0.6666 - loss: 0.6554 - val_accuracy: 1.0000 - val_loss: 0.1752\n",
            "Epoch 3/3\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.6574 - loss: 0.7740 - val_accuracy: 1.0000 - val_loss: 0.2952\n",
            " Using base model: MobileNetV2\n",
            "Epoch 1/3\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 3s/step - accuracy: 0.5466 - loss: 0.7462 - val_accuracy: 1.0000 - val_loss: 0.2078\n",
            "Epoch 2/3\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.4537 - loss: 1.0649 - val_accuracy: 1.0000 - val_loss: 0.2273\n",
            "Epoch 3/3\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.4425 - loss: 0.9342 - val_accuracy: 0.0000e+00 - val_loss: 1.6214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "print(\"model : \",model_fn.__name__)\n",
        "# Path to  single test video\n",
        "test_video_path = Path(\"/content/data/Real Life Violence Dataset/test/NonViolence/NV_101.mp4\")\n",
        "\n",
        "# 1. Extract frames from video\n",
        "frames = frames_from_video_file(test_video_path, n_frames=10)  # shape: (10, 224, 224, 3)\n",
        "\n",
        "# 2. Add a batch dimension: (1, 10, 224, 224, 3)\n",
        "frames = tf.expand_dims(frames, axis=0)\n",
        "\n",
        "# 3. Pass it to your model\n",
        "pred = model(frames)  # shape: (1, num_classes)\n",
        "\n",
        "# 4. Get predicted label\n",
        "pred_label = tf.argmax(pred, axis=-1).numpy()[0]\n",
        "print(\"Predicted class ID:\", pred_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMgpFQtlGRps",
        "outputId": "25a54041-d8b4-4b66-edbb-12867f2f85fb"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model :  MobileNetV2\n",
            "Predicted class ID: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#0: non violence"
      ],
      "metadata": {
        "id": "YqToGCeAJu_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Omgf7p36KBd9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}